{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................100\n",
      "....................................................................................................200\n",
      "....................................................................................................300\n",
      "....................................................................................................400\n",
      "....................................................................................................500\n",
      "....................................................................................................600\n",
      "....................................................................................................700\n",
      "....................................................................................................800\n",
      "....................................................................................................900\n",
      "....................................................................................................1000\n",
      "....................................................................................................1100\n",
      "....................................................................................................1200\n",
      "....................................................................................................1300\n",
      "........................"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pubchempy as pcp\n",
    "import time\n",
    "\n",
    "count = 0\n",
    "name_mernum_edge_list = []\n",
    "name_cid_edge_list = []\n",
    "cid_mernum_edge_list = []\n",
    "cid_smiles_edge_list = []\n",
    "\n",
    "no_inhibits_link_urls = []\n",
    "no_mernum_urls = []\n",
    "\n",
    "\n",
    "base = 'https://www.ebi.ac.uk'\n",
    "\n",
    "urls = set({})\n",
    "\n",
    "content = requests.get('https://www.ebi.ac.uk/merops/cgi-bin/smi_index').content\n",
    "\n",
    "soup = bs4.BeautifulSoup(content, 'lxml')\n",
    "table = soup.find('table')\n",
    "links = table.find_all('a') #All the links of the table\n",
    "#     print(links)\n",
    "links = {base + link.get('href') for link in links}\n",
    "urls.update(links)\n",
    "\n",
    "# print(urls)\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    try: \n",
    "        print('.', end = '')\n",
    "        count+=1\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        content = requests.get(url).content\n",
    "\n",
    "        soup = bs4.BeautifulSoup(content, 'lxml')\n",
    "\n",
    "        inhibit_link = soup.find_all(class_ = \"inhibit button\")\n",
    "\n",
    "        if len(inhibit_link) != 0:\n",
    "            inhibit_link = base + (inhibit_link[0].get('href')) #The link to the page listing the inhibitors \n",
    "\n",
    "        dt_tags = soup.find_all('dt'); #First one is always 'common name'. There are usually 'other names' and occasionally 'CID at pubchem'\n",
    "        dt_strings = []\n",
    "        other_names = []\n",
    "        for tag in dt_tags: #Storing the text for future checks. This array is parallel to dt_tags\n",
    "            dt_strings.append(tag.string)\n",
    "\n",
    "        cids = set()\n",
    "\n",
    "        try:\n",
    "            CID_index = dt_strings.index('CID at PubChem') #See if the CID is already on the page\n",
    "            cids.add(int(dt_tags[CID_index].find_next_siblings('dd')[0].string)) #Goes to the 'CID at PubChem' and finds the text following it\n",
    "                                                                                 #Casts the cid to an int so it's the same type as the later cids\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            other_names_index = dt_strings.index('Other names') #Store other names in case the common name yields no results at pubchem\n",
    "            other_names = dt_tags[other_names_index].find_next_siblings('dd')[0].string.split('; ') #Multiple names are separated by ; so split separates them into a list\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        common_name = dt_tags[0].find_next_siblings('dd')[0].string\n",
    "        compounds = pcp.get_compounds(common_name, 'name') #Stores all the compounds resulting from pubchempy lookup\n",
    "\n",
    "\n",
    "        for name in other_names:\n",
    "            cs_ = pcp.get_compounds(name, 'name') #get compound from each 'other name' and check for duplicates\n",
    "            for c_ in cs_: #To avoid duplicates (unfortunately I couldn't use a set here because compounds aren't hashable)\n",
    "                if c_ not in compounds:\n",
    "                    compounds.append(c_)\n",
    "\n",
    "\n",
    "        for compound in compounds: #Going to every compound in the list, find the cid and smiles\n",
    "            cid = compound.cid\n",
    "            cids.add(cid)\n",
    "            smiles = compound.isomeric_smiles\n",
    "            cid_smiles_edge_list.append((cid, smiles))\n",
    "\n",
    "    #     print(common_name, cids)\n",
    "\n",
    "        try:\n",
    "            inhibit_soup = bs4.BeautifulSoup(requests.get(inhibit_link).content, 'lxml')\n",
    "            inhibit_table = inhibit_soup.find('table').find_all('td')\n",
    "            for row in inhibit_table: #Go to each row of the inhitbit_table\n",
    "                for item in row.find_all('a'): #unfortunately includes refrences too, but we can just look for 'pepsum' in the link\n",
    "                    temp_link = item.get('href')\n",
    "                    if 'pepsum' in temp_link:\n",
    "                        mernum_link = base + temp_link\n",
    "                        mernum_soup = bs4.BeautifulSoup(requests.get(mernum_link).content, 'lxml') #Go to the merid page\n",
    "                        mernum_inlines = mernum_soup.find_all(class_ = 'inline')\n",
    "                        mernum = ''\n",
    "                        i = 0\n",
    "                        while 'MER' not in mernum: #The mernum is almost always the first element, but occasionally there are additional background links. This keeps looping until 'MER' text is found\n",
    "                            if i == len(mernum_inlines):\n",
    "                                break\n",
    "                            mernum = mernum_inlines[i].text\n",
    "                            i+=1\n",
    "                        if mernum == '': #If not mernum was found, skip the rest..\n",
    "                            no_mernum_urls.append(mernum_link)\n",
    "                            continue\n",
    "                        if len(cids) == 0: #All CIDs found are used in the edge list. If no CIDs were found, common name is used instead\n",
    "                            name_mernum_edge_list.append((common_name, mernum))\n",
    "                        else:\n",
    "                            for cid in cids:\n",
    "                                cid_mernum_edge_list.append((cid, mernum))\n",
    "        except:\n",
    "    #         print('oop ', url)\n",
    "            no_inhibits_link_urls.append(url)\n",
    "\n",
    "    #     merid_col = inhibit_table[0]['MERID']\n",
    "    #     for merid in merid_col:\n",
    "    #         name_merid_edge_list.append((common_name, merid))\n",
    "    except pcp.PubChemHTTPError:\n",
    "        print('Server Busy')\n",
    "        time.sleep(5) #Add slight delay to avoid overloading pcp servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count #The number of iterations / molecules in the initial table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mernum_edge_list = list(set(name_mernum_edge_list)) #cast the list to a set and back to a list to remove dupelicates\n",
    "name_cid_edge_list = list(set(name_cid_edge_list))\n",
    "cid_mernum_edge_list = list(set(cid_mernum_edge_list)) \n",
    "cid_smiles_edge_list = list(set(cid_smiles_edge_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "protease_file = open('D:/Downloads/protease.lib') #4th item on https://www.ebi.ac.uk/merops/download_list.shtml. Change this directory to appropriate location\n",
    "lines = protease_file.readlines()\n",
    "prev_mernum = lines.pop(0)[1:11]\n",
    "mernum_sequence_dict = {}\n",
    "prev_sequence = ''\n",
    "\n",
    "for line in lines:\n",
    "    count += len(line)\n",
    "    if line[1:4] == 'MER': #Checks if it's at a mernum line. If so, insert the mernum in the dictionary with the sequence and reset the sequence\n",
    "        mernum_sequence_dict[prev_mernum] = prev_sequence\n",
    "        prev_mernum = line[1:11]\n",
    "        prev_sequence = ''\n",
    "    else: #Otherwise keep adding to the previous sequence\n",
    "        prev_sequence+=line[:-2] #This chops off the '\\n' at the end of every line\n",
    "\n",
    "mernum_sequence_dict[prev_mernum] = prev_sequence #I update the dictionary at each new mernum line, so I have to manually insert the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #Pickle all of the data and store in seperate files\n",
    "pickle.dump(name_mernum_edge_list, open(\"name_mernum_edge_list.p\", \"wb\"))\n",
    "pickle.dump(name_cid_edge_list, open(\"name_cid_edge_list.p\", \"wb\"))\n",
    "pickle.dump(cid_mernum_edge_list, open(\"cid_mernum_edge_list.p\", \"wb\"))\n",
    "pickle.dump(cid_smiles_edge_list, open(\"cid_smiles_edge_list.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mernum_edge_list = pickle.load(open(\"name_mernum_edge_list.p\", \"rb\" )) #Opening a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2 \n",
    "with bz2.BZ2File('mernum_sequence_dict.pbz2', 'wb') as f: #This file is very large so I'm compressing it first\n",
    "    pickle.dump(mernum_sequence_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import bz2\n",
    "\n",
    "# Load any compressed pickle file\n",
    "def decompress_pickle(file):\n",
    "    data = bz2.BZ2File(file, 'rb')\n",
    "    data = cPickle.load(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mernum_sequence_dict = decompress_pickle('mernum_sequence_dict.pbz2') #To load the compressed pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
